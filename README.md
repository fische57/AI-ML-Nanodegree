# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains data about marketing for bank customers, with a binary yes or no outcome variable. We seek to predict when the outcome is yes or no.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

This solution used two different methods to find optimal models. The first was a Scikit-learn Pipeline to test the best combination of parameters for a SKLearn logistic regression model, where the best model yielded an accuracy of 0.9064. The second used AutoML to try many different regression models, evaluated by R2-score. The best AutoML model was MaxAbsScaler LightGBM with an R2-score of 0.4186. 

![image](https://github.com/fische57/AI-ML-Nanodegree/assets/52047242/cd9340bd-a6b9-4c08-999d-034e468111c5)

![image](https://github.com/fische57/AI-ML-Nanodegree/assets/52047242/166151b9-596d-4e44-9c33-4eace5583107)


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.** 

The Scikit-Learn Pipeline imported a TabularDataset of clean data, then used HyperDrive to automate the testing of two parameters: --C and --max_iter. --C is the inverse of regularization strenght, and max_iter is maximum number of iterations to converge. The values tested. A choice of three different values were tested for each parameter. Logistic regression is the algorithm of choice because of the binary outcome variable.

**What are the benefits of the parameter sampler you chose?**
A Random parameter sampler was chose which is good for a small number of choice options.

**What are the benefits of the early stopping policy you chose?**
The BanditPolicy keeps the program from running too long and taking too many resources.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.** 
The AutoML tested 21 different regression algorithms with unique customized pipelines. The best option for optimizing R2-score was MaxAbsScaler LightGBM. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?** 
Accuracy was not an accepted parameter for primary_metric in my AutoMLConfig, so I used R2-score. R2-score is not the ideal metric to measure a binary prediction, or to compare against accuracy, but it was accepted as a parameter so I ran what I could for the purpose of understanding the process. I learned a lot. While HyperDrive allowed for testing lots of parameters for one model, AutoML actually compared totally distinct algorithms with their own pipelines. This allows for a much broader analysis. If you already know which model is best suited for your problem, HyperDrive can zero in on the exact best parameters. If you want to explore what different ML algorithms can find in your data, AutoML performs that task very efficiently.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?** 
For future experiments, I would find a primary metric that matches between the two methods so they can be more directly compared. Also I would try using other data formats to see how the two options compare in different use cases.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion** 
The lab timed out. Resources were deleted automatically.
